{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2237a89-457c-4a14-96db-4b66cc3baa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [[\n",
    "    \"--epoch\", \"3\",\n",
    "    \"--head\", f\"{head}\",\n",
    "    \"--agg\", f\"{agg}\",\n",
    "    \"--learning_rate\", f\"{lr}\",\n",
    "] for head in [\n",
    "    \"BasicClassificationHead\", \"TwoLayerClassificationHead\"\n",
    "] for lr in [\n",
    "    3e-5, 1e-4\n",
    "] for agg in [\n",
    "    \"Aggregator\", \"AggregatorMax\", \"AggregatorMeanMax\", \"AggregatorMultiHead\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0302df3-2c96-4a89-be4f-5f050524cb95",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exp/bert-base-uncased_Aggregator_BasicClassificationHead/3*3e-05_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_Aggregator_BasicClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f5c427075e0>, log_path=None), max_length=100, model_name='bert-base-uncased_Aggregator_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Path exp/bert-base-uncased_AggregatorMax_BasicClassificationHead/3*3e-05_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMax_BasicClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f72be282550>, log_path=None), max_length=100, model_name='bert-base-uncased_AggregatorMax_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Path exp/bert-base-uncased_AggregatorMeanMax_BasicClassificationHead/3*3e-05_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMeanMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMeanMax_BasicClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f6c24a1f5e0>, log_path=None), max_length=100, model_name='bert-base-uncased_AggregatorMeanMax_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Create new experiment directory: exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*3e-05_per_32*1.\n",
      "Save the args-namespace at: exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*3e-05_per_32*1/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMultiHead', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f68875735e0>, log_path='exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*3e-05_per_32*1/log.txt'), max_length=100, model_name='bert-base-uncased_AggregatorMultiHead_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Mon Dec 27 19:13:10 2021\n",
      "Model(\n",
      "  (bert): bert-base-uncased\n",
      "  (aggregator): AggregatorMultiHead(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): BasicClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/3\n",
      "loss=1.14:  33%|█████████▉                    | 296/894 [00:28<00:58, 10.31it/s]Eval -- 298: \n",
      "loss = 1.2659754753112793\n",
      "update best eval loss to: 1.265975\n",
      "\n",
      "loss=0.98:  66%|███████████████████▉          | 594/894 [01:08<00:29, 10.21it/s]Eval -- 596: \n",
      "loss = 0.9810864925384521\n",
      "update best eval loss to: 0.981086\n",
      "\n",
      "loss=1.18: 100%|█████████████████████████████▉| 892/894 [01:47<00:00, 10.17it/s]Eval -- 894: \n",
      "loss = 0.8916864395141602\n",
      "update best eval loss to: 0.891686\n",
      "\n",
      "loss=1.18: 100%|██████████████████████████████| 894/894 [01:58<00:00,  7.56it/s]\n",
      "epoch 2/3\n",
      "loss=0.85:  33%|█████████▉                    | 296/894 [00:29<00:58, 10.17it/s]Eval -- 1192: \n",
      "loss = 0.788356602191925\n",
      "update best eval loss to: 0.788357\n",
      "\n",
      "loss=0.79:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.12it/s]Eval -- 1490: \n",
      "loss = 0.7826137542724609\n",
      "update best eval loss to: 0.782614\n",
      "\n",
      "loss=0.84: 100%|█████████████████████████████▉| 892/894 [01:48<00:00, 10.12it/s]Eval -- 1788: \n",
      "loss = 0.7294272780418396\n",
      "update best eval loss to: 0.729427\n",
      "\n",
      "loss=0.84: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.49it/s]\n",
      "epoch 3/3\n",
      "loss=0.85:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.11it/s]Eval -- 2086: \n",
      "loss = 0.7113282084465027\n",
      "update best eval loss to: 0.711328\n",
      "\n",
      "loss=0.39:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.10it/s]Eval -- 2384: \n",
      "loss = 0.7042137980461121\n",
      "update best eval loss to: 0.704214\n",
      "\n",
      "loss=0.57: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.11it/s]Eval -- 2682: \n",
      "loss = 0.6988661885261536\n",
      "update best eval loss to: 0.698866\n",
      "\n",
      "loss=0.57: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.47it/s]\n",
      "finish training: best_loss = 0.6988661885261536\n",
      "\n",
      "Path exp/bert-base-uncased_Aggregator_BasicClassificationHead/3*0.0001_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_Aggregator_BasicClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f3a336355e0>, log_path=None), max_length=100, model_name='bert-base-uncased_Aggregator_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Path exp/bert-base-uncased_AggregatorMax_BasicClassificationHead/3*0.0001_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMax_BasicClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f6fe6dc0550>, log_path=None), max_length=100, model_name='bert-base-uncased_AggregatorMax_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Path exp/bert-base-uncased_AggregatorMeanMax_BasicClassificationHead/3*0.0001_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMeanMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMeanMax_BasicClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f050b578670>, log_path=None), max_length=100, model_name='bert-base-uncased_AggregatorMeanMax_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Create new experiment directory: exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*0.0001_per_32*1.\n",
      "Save the args-namespace at: exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*0.0001_per_32*1/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMultiHead', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='BasicClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f685b4955e0>, log_path='exp/bert-base-uncased_AggregatorMultiHead_BasicClassificationHead/3*0.0001_per_32*1/log.txt'), max_length=100, model_name='bert-base-uncased_AggregatorMultiHead_BasicClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Mon Dec 27 19:19:22 2021\n",
      "Model(\n",
      "  (bert): bert-base-uncased\n",
      "  (aggregator): AggregatorMultiHead(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): BasicClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/3\n",
      "loss=1.11:  33%|█████████▉                    | 297/894 [00:29<00:58, 10.17it/s]Eval -- 298: \n",
      "loss = 1.217302680015564\n",
      "update best eval loss to: 1.217303\n",
      "\n",
      "loss=0.98:  67%|███████████████████▉          | 595/894 [01:08<00:29, 10.15it/s]Eval -- 596: \n",
      "loss = 1.0269492864608765\n",
      "update best eval loss to: 1.026949\n",
      "\n",
      "loss=1.12: 100%|█████████████████████████████▉| 893/894 [01:48<00:00, 10.11it/s]Eval -- 894: \n",
      "loss = 0.8708688616752625\n",
      "update best eval loss to: 0.870869\n",
      "\n",
      "loss=1.12: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.49it/s]\n",
      "epoch 2/3\n",
      "loss=0.86:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.10it/s]Eval -- 1192: \n",
      "loss = 0.7920762896537781\n",
      "update best eval loss to: 0.792076\n",
      "\n",
      "loss=0.66:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.10it/s]Eval -- 1490: \n",
      "loss = 0.7825936079025269\n",
      "update best eval loss to: 0.782594\n",
      "\n",
      "loss=0.77: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.10it/s]Eval -- 1788: \n",
      "loss = 0.7006446719169617\n",
      "update best eval loss to: 0.700645\n",
      "\n",
      "loss=0.77: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.46it/s]\n",
      "epoch 3/3\n",
      "loss=0.61:  33%|█████████▉                    | 297/894 [00:29<00:59, 10.12it/s]Eval -- 2086: \n",
      "loss = 0.6901392340660095\n",
      "update best eval loss to: 0.690139\n",
      "\n",
      "loss=0.40:  67%|███████████████████▉          | 595/894 [01:09<00:29, 10.11it/s]Eval -- 2384: \n",
      "loss = 0.6781834363937378\n",
      "update best eval loss to: 0.678183\n",
      "\n",
      "loss=0.54: 100%|█████████████████████████████▉| 893/894 [01:49<00:00, 10.10it/s]Eval -- 2682: \n",
      "loss = 0.676252007484436\n",
      "update best eval loss to: 0.676252\n",
      "\n",
      "loss=0.54: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.47it/s]\n",
      "finish training: best_loss = 0.676252007484436\n",
      "\n",
      "Path exp/bert-base-uncased_Aggregator_TwoLayerClassificationHead/3*3e-05_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_Aggregator_TwoLayerClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7fad70d85550>, log_path=None), max_length=100, model_name='bert-base-uncased_Aggregator_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Path exp/bert-base-uncased_AggregatorMax_TwoLayerClassificationHead/3*3e-05_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMax_TwoLayerClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f5fc7496550>, log_path=None), max_length=100, model_name='bert-base-uncased_AggregatorMax_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Create new experiment directory: exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*3e-05_per_32*1.\n",
      "Save the args-namespace at: exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*3e-05_per_32*1/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMeanMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7fded684d5e0>, log_path='exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*3e-05_per_32*1/log.txt'), max_length=100, model_name='bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Mon Dec 27 19:25:34 2021\n",
      "Model(\n",
      "  (bert): bert-base-uncased\n",
      "  (aggregator): AggregatorMeanMax(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=768, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/3\n",
      "loss=1.04:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.13it/s]Eval -- 298: \n",
      "loss = 1.244231104850769\n",
      "update best eval loss to: 1.244231\n",
      "\n",
      "loss=0.79:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.12it/s]Eval -- 596: \n",
      "loss = 0.9392449259757996\n",
      "update best eval loss to: 0.939245\n",
      "\n",
      "loss=1.29: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.09it/s]Eval -- 894: \n",
      "loss = 0.8504037857055664\n",
      "update best eval loss to: 0.850404\n",
      "\n",
      "loss=1.29: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.48it/s]\n",
      "epoch 2/3\n",
      "loss=0.88:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.10it/s]Eval -- 1192: \n",
      "loss = 0.7904747128486633\n",
      "update best eval loss to: 0.790475\n",
      "\n",
      "loss=0.77:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.07it/s]Eval -- 1490: \n",
      "loss = 0.7647577524185181\n",
      "update best eval loss to: 0.764758\n",
      "\n",
      "loss=1.28: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.08it/s]Eval -- 1788: \n",
      "loss = 0.7213344573974609\n",
      "update best eval loss to: 0.721334\n",
      "\n",
      "loss=1.28: 100%|██████████████████████████████| 894/894 [02:00<00:00,  7.45it/s]\n",
      "epoch 3/3\n",
      "loss=0.47:  33%|█████████▉                    | 297/894 [00:29<00:59, 10.08it/s]Eval -- 2086: \n",
      "loss = 0.7012587189674377\n",
      "update best eval loss to: 0.701259\n",
      "\n",
      "loss=0.53:  67%|███████████████████▉          | 595/894 [01:09<00:29, 10.07it/s]Eval -- 2384: \n",
      "loss = 0.6986000537872314\n",
      "update best eval loss to: 0.698600\n",
      "\n",
      "loss=0.32: 100%|█████████████████████████████▉| 893/894 [01:49<00:00, 10.07it/s]Eval -- 2682: \n",
      "loss = 0.6918810606002808\n",
      "update best eval loss to: 0.691881\n",
      "\n",
      "loss=0.32: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.45it/s]\n",
      "finish training: best_loss = 0.6918810606002808\n",
      "\n",
      "Create new experiment directory: exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*3e-05_per_32*1.\n",
      "Save the args-namespace at: exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*3e-05_per_32*1/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMultiHead', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*3e-05_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7fda0ead8670>, log_path='exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*3e-05_per_32*1/log.txt'), max_length=100, model_name='bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Mon Dec 27 19:31:42 2021\n",
      "Model(\n",
      "  (bert): bert-base-uncased\n",
      "  (aggregator): AggregatorMultiHead(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=768, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/3\n",
      "loss=1.12:  33%|█████████▉                    | 296/894 [00:29<00:58, 10.14it/s]Eval -- 298: \n",
      "loss = 1.3829935789108276\n",
      "update best eval loss to: 1.382994\n",
      "\n",
      "loss=0.89:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.10it/s]Eval -- 596: \n",
      "loss = 1.0152332782745361\n",
      "update best eval loss to: 1.015233\n",
      "\n",
      "loss=1.38: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.09it/s]Eval -- 894: \n",
      "loss = 0.8947427272796631\n",
      "update best eval loss to: 0.894743\n",
      "\n",
      "loss=1.38: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.47it/s]\n",
      "epoch 2/3\n",
      "loss=1.02:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.09it/s]Eval -- 1192: \n",
      "loss = 0.8544890880584717\n",
      "update best eval loss to: 0.854489\n",
      "\n",
      "loss=0.75:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.10it/s]Eval -- 1490: \n",
      "loss = 0.7920452356338501\n",
      "update best eval loss to: 0.792045\n",
      "\n",
      "loss=1.24: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.08it/s]Eval -- 1788: \n",
      "loss = 0.765012800693512\n",
      "update best eval loss to: 0.765013\n",
      "\n",
      "loss=1.24: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.46it/s]\n",
      "epoch 3/3\n",
      "loss=0.49:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.08it/s]Eval -- 2086: \n",
      "loss = 0.7319350838661194\n",
      "update best eval loss to: 0.731935\n",
      "\n",
      "loss=0.63:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.09it/s]Eval -- 2384: \n",
      "loss = 0.7239770293235779\n",
      "update best eval loss to: 0.723977\n",
      "\n",
      "loss=0.35: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.10it/s]Eval -- 2682: \n",
      "loss = 0.7221773266792297\n",
      "update best eval loss to: 0.722177\n",
      "\n",
      "loss=0.35: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.46it/s]\n",
      "finish training: best_loss = 0.7221773266792297\n",
      "\n",
      "Path exp/bert-base-uncased_Aggregator_TwoLayerClassificationHead/3*0.0001_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_Aggregator_TwoLayerClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7fcea4efe5e0>, log_path=None), max_length=100, model_name='bert-base-uncased_Aggregator_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Path exp/bert-base-uncased_AggregatorMax_TwoLayerClassificationHead/3*0.0001_per_32*1 exists. Skip this experiment.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMax_TwoLayerClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f98bf080670>, log_path=None), max_length=100, model_name='bert-base-uncased_AggregatorMax_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=True, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "Create new experiment directory: exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*0.0001_per_32*1.\n",
      "Save the args-namespace at: exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*0.0001_per_32*1/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMeanMax', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7ff5e1d61670>, log_path='exp/bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead/3*0.0001_per_32*1/log.txt'), max_length=100, model_name='bert-base-uncased_AggregatorMeanMax_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Mon Dec 27 19:37:55 2021\n",
      "Model(\n",
      "  (bert): bert-base-uncased\n",
      "  (aggregator): AggregatorMeanMax(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=768, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/3\n",
      "loss=1.01:  33%|█████████▉                    | 297/894 [00:29<00:58, 10.14it/s]Eval -- 298: \n",
      "loss = 1.1906750202178955\n",
      "update best eval loss to: 1.190675\n",
      "\n",
      "loss=0.90:  67%|███████████████████▉          | 595/894 [01:09<00:29, 10.09it/s]Eval -- 596: \n",
      "loss = 0.9299572110176086\n",
      "update best eval loss to: 0.929957\n",
      "\n",
      "loss=1.15: 100%|█████████████████████████████▉| 893/894 [01:49<00:00, 10.08it/s]Eval -- 894: \n",
      "loss = 0.8332219123840332\n",
      "update best eval loss to: 0.833222\n",
      "\n",
      "loss=1.15: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.47it/s]\n",
      "epoch 2/3\n",
      "loss=0.76:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.08it/s]Eval -- 1192: \n",
      "loss = 0.8132383823394775\n",
      "update best eval loss to: 0.813238\n",
      "\n",
      "loss=0.73:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.07it/s]Eval -- 1490: \n",
      "loss = 0.7392174005508423\n",
      "update best eval loss to: 0.739217\n",
      "\n",
      "loss=1.46: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.08it/s]Eval -- 1788: \n",
      "loss = 0.7114306092262268\n",
      "update best eval loss to: 0.711431\n",
      "\n",
      "loss=1.46: 100%|██████████████████████████████| 894/894 [02:00<00:00,  7.45it/s]\n",
      "epoch 3/3\n",
      "loss=0.46:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.06it/s]Eval -- 2086: \n",
      "loss = 0.6825905442237854\n",
      "update best eval loss to: 0.682591\n",
      "\n",
      "loss=0.42:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.08it/s]Eval -- 2384: \n",
      "loss = 0.6694612503051758\n",
      "update best eval loss to: 0.669461\n",
      "\n",
      "loss=0.27: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.08it/s]Eval -- 2682: \n",
      "loss = 0.6656298041343689\n",
      "update best eval loss to: 0.665630\n",
      "\n",
      "loss=0.27: 100%|██████████████████████████████| 894/894 [02:00<00:00,  7.45it/s]\n",
      "finish training: best_loss = 0.6656298041343689\n",
      "\n",
      "Create new experiment directory: exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*0.0001_per_32*1.\n",
      "Save the args-namespace at: exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*0.0001_per_32*1/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='AggregatorMultiHead', batch_size=32, bias_sampling=False, epoch=3, erase=False, exp_dir='exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*0.0001_per_32*1', freeze_pretrained=False, gradient_accumulation_steps=1, head_class='TwoLayerClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f9d167fc5e0>, log_path='exp/bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead/3*0.0001_per_32*1/log.txt'), max_length=100, model_name='bert-base-uncased_AggregatorMultiHead_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='bert-base-uncased', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Mon Dec 27 19:44:03 2021\n",
      "Model(\n",
      "  (bert): bert-base-uncased\n",
      "  (aggregator): AggregatorMultiHead(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=768, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/3\n",
      "loss=1.14:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.10it/s]Eval -- 298: \n",
      "loss = 1.3053514957427979\n",
      "update best eval loss to: 1.305351\n",
      "\n",
      "loss=0.98:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.09it/s]Eval -- 596: \n",
      "loss = 1.0177313089370728\n",
      "update best eval loss to: 1.017731\n",
      "\n",
      "loss=1.23: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.09it/s]Eval -- 894: \n",
      "loss = 0.8965925574302673\n",
      "update best eval loss to: 0.896593\n",
      "\n",
      "loss=1.23: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.46it/s]\n",
      "epoch 2/3\n",
      "loss=1.09:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.11it/s]Eval -- 1192: \n",
      "loss = 0.8683638572692871\n",
      "update best eval loss to: 0.868364\n",
      "\n",
      "loss=0.66:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.10it/s]Eval -- 1490: \n",
      "loss = 0.7939496636390686\n",
      "update best eval loss to: 0.793950\n",
      "\n",
      "loss=1.11: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.09it/s]Eval -- 1788: \n",
      "loss = 0.7331592440605164\n",
      "update best eval loss to: 0.733159\n",
      "\n",
      "loss=1.11: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.46it/s]\n",
      "epoch 3/3\n",
      "loss=0.50:  33%|█████████▉                    | 296/894 [00:29<00:59, 10.10it/s]Eval -- 2086: \n",
      "loss = 0.711540162563324\n",
      "update best eval loss to: 0.711540\n",
      "\n",
      "loss=0.45:  66%|███████████████████▉          | 594/894 [01:09<00:29, 10.09it/s]Eval -- 2384: \n",
      "loss = 0.6999603509902954\n",
      "update best eval loss to: 0.699960\n",
      "\n",
      "loss=0.22: 100%|█████████████████████████████▉| 892/894 [01:49<00:00, 10.08it/s]Eval -- 2682: \n",
      "loss = 0.6959571838378906\n",
      "update best eval loss to: 0.695957\n",
      "\n",
      "loss=0.22: 100%|██████████████████████████████| 894/894 [01:59<00:00,  7.46it/s]\n",
      "finish training: best_loss = 0.6959571838378906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for args in args_list:\n",
    "    cmd = f\"python3 experiment.py {' '.join(args)}\"\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a64226-a37f-49d9-8945-0db32c442dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [[\n",
    "    \"--epoch\", \"10\",\n",
    "    \"--head\", f\"{head}\",\n",
    "    \"--agg\", f\"{agg}\",\n",
    "    \"--learning_rate\", f\"{lr}\",\n",
    "    \"--pretrain\", \"microsoft/deberta-v3-large\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    \"--gradient\", \"2\",\n",
    "    \"--erase\"\n",
    "] for head in [\n",
    "    #\"BasicClassificationHead\",\n",
    "    \"TwoLayerClassificationHead\"\n",
    "] for lr in [\n",
    "    1e-5, 3e-5, 1e-4\n",
    "] for agg in [\n",
    "    \"Aggregator\",\n",
    "    #\"AggregatorMultiHead\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c725d4-7257-44bb-a85e-062d62851a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new experiment directory: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*1e-05_per_16*2.\n",
      "Save the args-namespace at: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*1e-05_per_16*2/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=16, bias_sampling=False, epoch=10, erase=True, exp_dir='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*1e-05_per_16*2', freeze_pretrained=False, gradient_accumulation_steps=2, head_class='TwoLayerClassificationHead', learning_rate=1e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f1e6f40e700>, log_path='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*1e-05_per_16*2/log.txt'), max_length=100, model_name='microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='microsoft/deberta-v3-large', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Thu Dec 30 11:41:39 2021\n",
      "Model(\n",
      "  (bert): microsoft/deberta-v3-large\n",
      "  (aggregator): Aggregator(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=16384, out_features=16384, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=16384, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/10\n",
      "loss=1.91:  33%|█████████▋                   | 661/1988 [02:54<05:30,  4.02it/s]Eval -- 662: \n",
      "loss = 1.7336688041687012\n",
      "update best eval loss to: 1.733669\n",
      "\n",
      "loss=1.60:  67%|██████████████████▋         | 1323/1988 [06:47<02:45,  4.01it/s]Eval -- 1324: \n",
      "loss = 1.1898914575576782\n",
      "update best eval loss to: 1.189891\n",
      "\n",
      "loss=1.72: 100%|███████████████████████████▉| 1985/1988 [10:39<00:00,  4.01it/s]Eval -- 1986: \n",
      "loss = 1.0635906457901\n",
      "update best eval loss to: 1.063591\n",
      "\n",
      "loss=0.77: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 2/10\n",
      "loss=0.70:  33%|█████████▌                   | 659/1988 [02:54<05:31,  4.01it/s]Eval -- 2648: \n",
      "loss = 0.954273521900177\n",
      "update best eval loss to: 0.954274\n",
      "\n",
      "loss=0.95:  66%|██████████████████▌         | 1321/1988 [06:47<02:46,  4.01it/s]Eval -- 3310: \n",
      "loss = 0.8306604623794556\n",
      "update best eval loss to: 0.830660\n",
      "\n",
      "loss=0.73: 100%|███████████████████████████▉| 1983/1988 [10:39<00:01,  4.01it/s]Eval -- 3972: \n",
      "loss = 0.7865804433822632\n",
      "update best eval loss to: 0.786580\n",
      "\n",
      "loss=0.77: 100%|████████████████████████████| 1988/1988 [11:38<00:00,  2.85it/s]\n",
      "epoch 3/10\n",
      "loss=0.64:  33%|█████████▌                   | 657/1988 [02:54<05:31,  4.01it/s]Eval -- 4634: \n",
      "loss = 0.7002559304237366\n",
      "update best eval loss to: 0.700256\n",
      "\n",
      "loss=0.64:  66%|██████████████████▌         | 1319/1988 [06:46<02:46,  4.01it/s]Eval -- 5296: \n",
      "loss = 0.6860045790672302\n",
      "update best eval loss to: 0.686005\n",
      "\n",
      "loss=0.80: 100%|███████████████████████████▉| 1981/1988 [10:39<00:01,  4.01it/s]Eval -- 5958: \n",
      "loss = 0.6347644925117493\n",
      "update best eval loss to: 0.634764\n",
      "\n",
      "loss=0.52: 100%|████████████████████████████| 1988/1988 [11:38<00:00,  2.85it/s]\n",
      "epoch 4/10\n",
      "loss=0.42:  33%|█████████▌                   | 655/1988 [02:53<05:32,  4.01it/s]Eval -- 6620: \n",
      "loss = 0.5659299492835999\n",
      "update best eval loss to: 0.565930\n",
      "\n",
      "loss=0.68:  66%|██████████████████▌         | 1317/1988 [06:46<02:47,  4.01it/s]Eval -- 7282: \n",
      "loss = 0.5674950480461121\n",
      "loss=0.70: 100%|███████████████████████████▊| 1979/1988 [10:38<00:02,  4.01it/s]Eval -- 7944: \n",
      "loss = 0.5097331404685974\n",
      "update best eval loss to: 0.509733\n",
      "\n",
      "loss=0.85: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 5/10\n",
      "loss=0.44:  33%|█████████▌                   | 653/1988 [02:53<05:32,  4.01it/s]Eval -- 8606: \n",
      "loss = 0.48246410489082336\n",
      "update best eval loss to: 0.482464\n",
      "\n",
      "loss=0.68:  66%|██████████████████▌         | 1315/1988 [06:45<02:47,  4.01it/s]Eval -- 9268: \n",
      "loss = 0.47014090418815613\n",
      "update best eval loss to: 0.470141\n",
      "\n",
      "loss=0.76:  99%|███████████████████████████▊| 1977/1988 [10:38<00:02,  4.02it/s]Eval -- 9930: \n",
      "loss = 0.4616328179836273\n",
      "update best eval loss to: 0.461633\n",
      "\n",
      "loss=1.06: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 6/10\n",
      "loss=0.14:  33%|█████████▍                   | 651/1988 [02:52<05:32,  4.02it/s]Eval -- 10592: \n",
      "loss = 0.40778571367263794\n",
      "update best eval loss to: 0.407786\n",
      "\n",
      "loss=0.86:  66%|██████████████████▍         | 1313/1988 [06:44<02:48,  4.02it/s]Eval -- 11254: \n",
      "loss = 0.37210020422935486\n",
      "update best eval loss to: 0.372100\n",
      "\n",
      "loss=0.41:  99%|███████████████████████████▊| 1975/1988 [10:36<00:03,  4.02it/s]Eval -- 11916: \n",
      "loss = 0.3635543882846832\n",
      "update best eval loss to: 0.363554\n",
      "\n",
      "loss=0.41: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 7/10\n",
      "loss=0.27:  33%|█████████▍                   | 649/1988 [02:51<05:33,  4.01it/s]Eval -- 12578: \n",
      "loss = 0.31603363156318665\n",
      "update best eval loss to: 0.316034\n",
      "\n",
      "loss=0.65:  66%|██████████████████▍         | 1311/1988 [06:44<02:48,  4.01it/s]Eval -- 13240: \n",
      "loss = 0.2826562225818634\n",
      "update best eval loss to: 0.282656\n",
      "\n",
      "loss=0.21:  99%|███████████████████████████▊| 1973/1988 [10:36<00:03,  4.01it/s]Eval -- 13902: \n",
      "loss = 0.2615244686603546\n",
      "update best eval loss to: 0.261524\n",
      "\n",
      "loss=0.74: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 8/10\n",
      "loss=0.29:  33%|█████████▍                   | 647/1988 [02:51<05:34,  4.01it/s]Eval -- 14564: \n",
      "loss = 0.25813111662864685\n",
      "update best eval loss to: 0.258131\n",
      "\n",
      "loss=0.44:  66%|██████████████████▍         | 1309/1988 [06:43<02:49,  4.01it/s]Eval -- 15226: \n",
      "loss = 0.24994532763957977\n",
      "update best eval loss to: 0.249945\n",
      "\n",
      "loss=0.24:  99%|███████████████████████████▊| 1971/1988 [10:36<00:04,  4.02it/s]Eval -- 15888: \n",
      "loss = 0.19901517033576965\n",
      "update best eval loss to: 0.199015\n",
      "\n",
      "loss=0.36: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 9/10\n",
      "loss=0.18:  32%|█████████▍                   | 645/1988 [02:50<05:34,  4.02it/s]Eval -- 16550: \n",
      "loss = 0.1969008445739746\n",
      "update best eval loss to: 0.196901\n",
      "\n",
      "loss=0.39:  66%|██████████████████▍         | 1307/1988 [06:43<02:49,  4.01it/s]Eval -- 17212: \n",
      "loss = 0.1771172285079956\n",
      "update best eval loss to: 0.177117\n",
      "\n",
      "loss=0.62:  99%|███████████████████████████▋| 1969/1988 [10:35<00:04,  4.02it/s]Eval -- 17874: \n",
      "loss = 0.13729792833328247\n",
      "update best eval loss to: 0.137298\n",
      "\n",
      "loss=0.69: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 10/10\n",
      "loss=0.26:  32%|█████████▍                   | 643/1988 [02:50<05:34,  4.02it/s]Eval -- 18536: \n",
      "loss = 0.12386183440685272\n",
      "update best eval loss to: 0.123862\n",
      "\n",
      "loss=0.31:  66%|██████████████████▍         | 1305/1988 [06:42<02:50,  4.01it/s]Eval -- 19198: \n",
      "loss = 0.0937085822224617\n",
      "update best eval loss to: 0.093709\n",
      "\n",
      "loss=0.14:  99%|███████████████████████████▋| 1967/1988 [10:35<00:05,  4.01it/s]Eval -- 19860: \n",
      "loss = 0.09397152066230774\n",
      "loss=0.13: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "finish training: best_loss = 0.0937085822224617\n",
      "\n",
      "Create new experiment directory: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-05_per_16*2.\n",
      "Save the args-namespace at: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-05_per_16*2/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=16, bias_sampling=False, epoch=10, erase=True, exp_dir='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-05_per_16*2', freeze_pretrained=False, gradient_accumulation_steps=2, head_class='TwoLayerClassificationHead', learning_rate=3e-05, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f2d10913700>, log_path='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-05_per_16*2/log.txt'), max_length=100, model_name='microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='microsoft/deberta-v3-large', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Thu Dec 30 13:38:10 2021\n",
      "Model(\n",
      "  (bert): microsoft/deberta-v3-large\n",
      "  (aggregator): Aggregator(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=16384, out_features=16384, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=16384, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/10\n",
      "loss=1.60:  33%|█████████▋                   | 661/1988 [02:55<05:30,  4.01it/s]Eval -- 662: \n",
      "loss = 1.2876908779144287\n",
      "update best eval loss to: 1.287691\n",
      "\n",
      "loss=1.30:  67%|██████████████████▋         | 1323/1988 [06:47<02:45,  4.01it/s]Eval -- 1324: \n",
      "loss = 1.008275032043457\n",
      "update best eval loss to: 1.008275\n",
      "\n",
      "loss=1.44: 100%|███████████████████████████▉| 1985/1988 [10:40<00:00,  4.01it/s]Eval -- 1986: \n",
      "loss = 0.871612548828125\n",
      "update best eval loss to: 0.871613\n",
      "\n",
      "loss=0.38: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 2/10\n",
      "loss=0.50:  33%|█████████▌                   | 659/1988 [02:54<05:31,  4.01it/s]Eval -- 2648: \n",
      "loss = 0.8182461857795715\n",
      "update best eval loss to: 0.818246\n",
      "\n",
      "loss=1.01:  66%|██████████████████▌         | 1321/1988 [06:47<02:46,  4.01it/s]Eval -- 3310: \n",
      "loss = 0.7342042922973633\n",
      "update best eval loss to: 0.734204\n",
      "\n",
      "loss=0.64: 100%|███████████████████████████▉| 1983/1988 [10:39<00:01,  4.02it/s]Eval -- 3972: \n",
      "loss = 0.7514049410820007\n",
      "loss=0.86: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 3/10\n",
      "loss=0.57:  33%|█████████▌                   | 657/1988 [02:54<05:31,  4.02it/s]Eval -- 4634: \n",
      "loss = 0.627731442451477\n",
      "update best eval loss to: 0.627731\n",
      "\n",
      "loss=0.56:  66%|██████████████████▌         | 1319/1988 [06:46<02:46,  4.02it/s]Eval -- 5296: \n",
      "loss = 0.5947830080986023\n",
      "update best eval loss to: 0.594783\n",
      "\n",
      "loss=0.46: 100%|███████████████████████████▉| 1981/1988 [10:38<00:01,  4.02it/s]Eval -- 5958: \n",
      "loss = 0.6270987391471863\n",
      "loss=0.60: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 4/10\n",
      "loss=0.36:  33%|█████████▌                   | 655/1988 [02:53<05:31,  4.02it/s]Eval -- 6620: \n",
      "loss = 0.5550748109817505\n",
      "update best eval loss to: 0.555075\n",
      "\n",
      "loss=0.79:  66%|██████████████████▌         | 1317/1988 [06:45<02:47,  4.02it/s]Eval -- 7282: \n",
      "loss = 0.4867153465747833\n",
      "update best eval loss to: 0.486715\n",
      "\n",
      "loss=0.46: 100%|███████████████████████████▊| 1979/1988 [10:37<00:02,  4.02it/s]Eval -- 7944: \n",
      "loss = 0.4876783490180969\n",
      "loss=0.77: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 5/10\n",
      "loss=2.68:  33%|█████████▌                   | 653/1988 [02:52<05:32,  4.02it/s]Eval -- 8606: \n",
      "loss = 2.625150680541992\n",
      "loss=2.76:  66%|██████████████████▌         | 1315/1988 [06:44<02:47,  4.02it/s]Eval -- 9268: \n",
      "loss = 2.6300692558288574\n",
      "loss=2.31:  99%|███████████████████████████▊| 1977/1988 [10:36<00:02,  4.02it/s]Eval -- 9930: \n",
      "loss = 2.6199936866760254\n",
      "loss=2.90: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 6/10\n",
      "loss=2.15:  33%|█████████▍                   | 651/1988 [02:52<05:32,  4.02it/s]Eval -- 10592: \n",
      "loss = 2.6142425537109375\n",
      "loss=2.38:  66%|██████████████████▍         | 1313/1988 [06:44<02:47,  4.02it/s]Eval -- 11254: \n",
      "loss = 2.6170904636383057\n",
      "loss=2.30:  99%|███████████████████████████▊| 1975/1988 [10:36<00:03,  4.02it/s]Eval -- 11916: \n",
      "loss = 2.6233441829681396\n",
      "loss=2.63: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 7/10\n",
      "loss=2.51:  33%|█████████▍                   | 649/1988 [02:51<05:33,  4.02it/s]Eval -- 12578: \n",
      "loss = 2.6137633323669434\n",
      "loss=2.72:  66%|██████████████████▍         | 1311/1988 [06:43<02:48,  4.02it/s]Eval -- 13240: \n",
      "loss = 2.61318039894104\n",
      "loss=2.64:  99%|███████████████████████████▊| 1973/1988 [10:35<00:03,  4.02it/s]Eval -- 13902: \n",
      "loss = 2.6160266399383545\n",
      "loss=2.95: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 8/10\n",
      "loss=3.27:  33%|█████████▍                   | 647/1988 [02:51<05:33,  4.02it/s]Eval -- 14564: \n",
      "loss = 2.608592987060547\n",
      "loss=2.59:  66%|██████████████████▍         | 1309/1988 [06:43<02:48,  4.02it/s]Eval -- 15226: \n",
      "loss = 2.610403537750244\n",
      "loss=2.21:  99%|███████████████████████████▊| 1971/1988 [10:35<00:04,  4.02it/s]Eval -- 15888: \n",
      "loss = 2.608612298965454\n",
      "loss=2.69: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 9/10\n",
      "loss=2.44:  32%|█████████▍                   | 645/1988 [02:50<05:34,  4.02it/s]Eval -- 16550: \n",
      "loss = 2.6143829822540283\n",
      "loss=2.44:  66%|██████████████████▍         | 1307/1988 [06:42<02:49,  4.02it/s]Eval -- 17212: \n",
      "loss = 2.6084671020507812\n",
      "loss=2.67:  99%|███████████████████████████▋| 1969/1988 [10:35<00:04,  4.02it/s]Eval -- 17874: \n",
      "loss = 2.604933500289917\n",
      "loss=2.37: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 10/10\n",
      "loss=2.50:  32%|█████████▍                   | 643/1988 [02:50<05:34,  4.02it/s]Eval -- 18536: \n",
      "loss = 2.613271713256836\n",
      "loss=2.69:  66%|██████████████████▍         | 1305/1988 [06:42<02:50,  4.02it/s]Eval -- 19198: \n",
      "loss = 2.6120128631591797\n",
      "loss=2.53:  99%|███████████████████████████▋| 1967/1988 [10:34<00:05,  4.02it/s]Eval -- 19860: \n",
      "loss = 2.6086313724517822\n",
      "loss=2.66: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "finish training: best_loss = 0.4867153465747833\n",
      "\n",
      "Create new experiment directory: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*0.0001_per_16*2.\n",
      "Save the args-namespace at: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*0.0001_per_16*2/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=16, bias_sampling=False, epoch=10, erase=True, exp_dir='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*0.0001_per_16*2', freeze_pretrained=False, gradient_accumulation_steps=2, head_class='TwoLayerClassificationHead', learning_rate=0.0001, logging=functools.partial(<function create_logging.<locals>.logging at 0x7f642a624700>, log_path='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*0.0001_per_16*2/log.txt'), max_length=100, model_name='microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='microsoft/deberta-v3-large', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Thu Dec 30 15:34:35 2021\n",
      "Model(\n",
      "  (bert): microsoft/deberta-v3-large\n",
      "  (aggregator): Aggregator(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=16384, out_features=16384, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=16384, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/10\n",
      "loss=1.68:  33%|█████████▋                   | 661/1988 [02:55<05:30,  4.01it/s]Eval -- 662: \n",
      "loss = 1.2007721662521362\n",
      "update best eval loss to: 1.200772\n",
      "\n",
      "loss=1.52:  67%|██████████████████▋         | 1323/1988 [06:47<02:45,  4.01it/s]Eval -- 1324: \n",
      "loss = 1.0132957696914673\n",
      "update best eval loss to: 1.013296\n",
      "\n",
      "loss=1.08: 100%|███████████████████████████▉| 1985/1988 [10:40<00:00,  4.01it/s]Eval -- 1986: \n",
      "loss = 0.8627536296844482\n",
      "update best eval loss to: 0.862754\n",
      "\n",
      "loss=0.30: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 2/10\n",
      "loss=0.77:  33%|█████████▌                   | 659/1988 [02:54<05:30,  4.02it/s]Eval -- 2648: \n",
      "loss = 0.827695906162262\n",
      "update best eval loss to: 0.827696\n",
      "\n",
      "loss=1.30:  66%|██████████████████▌         | 1321/1988 [06:46<02:45,  4.02it/s]Eval -- 3310: \n",
      "loss = 0.8296000957489014\n",
      "loss=0.77: 100%|███████████████████████████▉| 1983/1988 [10:38<00:01,  4.02it/s]Eval -- 3972: \n",
      "loss = 0.9430298209190369\n",
      "loss=1.38: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 3/10\n",
      "loss=2.55:  33%|█████████▌                   | 657/1988 [02:53<05:31,  4.02it/s]Eval -- 4634: \n",
      "loss = 2.6450963020324707\n",
      "loss=2.82:  66%|██████████████████▌         | 1319/1988 [06:45<02:46,  4.02it/s]Eval -- 5296: \n",
      "loss = 2.61555814743042\n",
      "loss=2.54: 100%|███████████████████████████▉| 1981/1988 [10:37<00:01,  4.02it/s]Eval -- 5958: \n",
      "loss = 2.6058244705200195\n",
      "loss=2.54: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 4/10\n",
      "loss=2.38:  33%|█████████▌                   | 655/1988 [02:53<05:31,  4.02it/s]Eval -- 6620: \n",
      "loss = 2.6110892295837402\n",
      "loss=2.42:  66%|██████████████████▌         | 1317/1988 [06:45<02:47,  4.02it/s]Eval -- 7282: \n",
      "loss = 2.60714054107666\n",
      "loss=2.42: 100%|███████████████████████████▊| 1979/1988 [10:37<00:02,  4.02it/s]Eval -- 7944: \n",
      "loss = 2.6057889461517334\n",
      "loss=2.77: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 5/10\n",
      "loss=2.77:  33%|█████████▌                   | 653/1988 [02:52<05:32,  4.01it/s]Eval -- 8606: \n",
      "loss = 2.6180408000946045\n",
      "loss=2.74:  66%|██████████████████▌         | 1315/1988 [06:44<02:47,  4.02it/s]Eval -- 9268: \n",
      "loss = 2.608032464981079\n",
      "loss=2.31:  99%|███████████████████████████▊| 1977/1988 [10:37<00:02,  4.02it/s]Eval -- 9930: \n",
      "loss = 2.609598159790039\n",
      "loss=2.93: 100%|████████████████████████████| 1988/1988 [11:36<00:00,  2.85it/s]\n",
      "epoch 6/10\n",
      "loss=2.18:  33%|█████████▍                   | 651/1988 [02:52<05:32,  4.02it/s]Eval -- 10592: \n",
      "loss = 2.6056857109069824\n",
      "loss=2.39:  66%|██████████████████▍         | 1313/1988 [06:44<02:48,  4.02it/s]Eval -- 11254: \n",
      "loss = 2.605783224105835\n",
      "loss=2.28:  99%|███████████████████████████▊| 1975/1988 [10:36<00:03,  4.02it/s]Eval -- 11916: \n",
      "loss = 2.6144683361053467\n",
      "loss=2.62: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 7/10\n",
      "loss=2.51:  33%|█████████▍                   | 649/1988 [02:51<05:33,  4.02it/s]Eval -- 12578: \n",
      "loss = 2.6063425540924072\n",
      "loss=2.71:  66%|██████████████████▍         | 1311/1988 [06:44<02:48,  4.02it/s]Eval -- 13240: \n",
      "loss = 2.6073718070983887\n",
      "loss=2.62:  99%|███████████████████████████▊| 1973/1988 [10:36<00:03,  4.01it/s]Eval -- 13902: \n",
      "loss = 2.608633279800415\n",
      "loss=2.97: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 8/10\n",
      "loss=3.19:  33%|█████████▍                   | 647/1988 [02:51<05:34,  4.01it/s]Eval -- 14564: \n",
      "loss = 2.6043472290039062\n",
      "loss=2.57:  66%|██████████████████▍         | 1309/1988 [06:43<02:49,  4.01it/s]Eval -- 15226: \n",
      "loss = 2.6051833629608154\n",
      "loss=2.27:  99%|███████████████████████████▊| 1971/1988 [10:35<00:04,  4.01it/s]Eval -- 15888: \n",
      "loss = 2.6038076877593994\n",
      "loss=2.64: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 9/10\n",
      "loss=2.45:  32%|█████████▍                   | 645/1988 [02:50<05:34,  4.01it/s]Eval -- 16550: \n",
      "loss = 2.608130931854248\n",
      "loss=2.44:  66%|██████████████████▍         | 1307/1988 [06:43<02:49,  4.02it/s]Eval -- 17212: \n",
      "loss = 2.6100263595581055\n",
      "loss=2.66:  99%|███████████████████████████▋| 1969/1988 [10:35<00:04,  4.01it/s]Eval -- 17874: \n",
      "loss = 2.6042356491088867\n",
      "loss=2.36: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 10/10\n",
      "loss=2.51:  32%|█████████▍                   | 643/1988 [02:50<05:35,  4.01it/s]Eval -- 18536: \n",
      "loss = 2.6078267097473145\n",
      "loss=2.67:  66%|██████████████████▍         | 1305/1988 [06:42<02:50,  4.01it/s]Eval -- 19198: \n",
      "loss = 2.60556960105896\n",
      "loss=2.53:  99%|███████████████████████████▋| 1967/1988 [10:35<00:05,  4.01it/s]Eval -- 19860: \n",
      "loss = 2.611354112625122\n",
      "loss=2.66: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "finish training: best_loss = 0.827695906162262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for args in args_list:\n",
    "    cmd = f\"python3 experiment.py {' '.join(args)}\"\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee8cb77-24ff-4dc4-ae9c-fc698a512994",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [[\n",
    "    \"--epoch\", \"10\",\n",
    "    \"--head\", f\"{head}\",\n",
    "    \"--agg\", f\"{agg}\",\n",
    "    \"--learning_rate\", f\"{lr}\",\n",
    "    \"--pretrain\", \"microsoft/deberta-v3-large\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    \"--gradient\", \"2\",\n",
    "    \"--erase\"\n",
    "] for head in [\n",
    "    #\"BasicClassificationHead\",\n",
    "    \"TwoLayerClassificationHead\"\n",
    "] for lr in [\n",
    "    3e-6,\n",
    "] for agg in [\n",
    "    \"Aggregator\",\n",
    "    #\"AggregatorMultiHead\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac33e4a-bd3b-47d8-9d69-16244fceef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new experiment directory: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-06_per_16*2.\n",
      "Save the args-namespace at: exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-06_per_16*2/args.pt.\n",
      "Namespace(adam_epsilon=1e-08, agg_class='Aggregator', batch_size=16, bias_sampling=False, epoch=10, erase=True, exp_dir='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-06_per_16*2', freeze_pretrained=False, gradient_accumulation_steps=2, head_class='TwoLayerClassificationHead', learning_rate=3e-06, logging=functools.partial(<function create_logging.<locals>.logging at 0x7fac96306790>, log_path='exp/microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead/10*3e-06_per_16*2/log.txt'), max_length=100, model_name='microsoft/deberta-v3-large_Aggregator_TwoLayerClassificationHead', opt='adamw', pretrained_model_name_or_path='microsoft/deberta-v3-large', scheduler_style='static', seed=123456, skip_experiment=False, test=False, tiny_experiment=False, weight_decay=0.01)\n",
      "This experiemnt started at: Thu Dec 30 17:31:01 2021\n",
      "Model(\n",
      "  (bert): microsoft/deberta-v3-large\n",
      "  (aggregator): Aggregator(\n",
      "    (hidden_states_to_attention_scores): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (head): TwoLayerClassificationHead(\n",
      "    (MLP): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=16384, out_features=16384, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=16384, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "begin training\n",
      "epoch 1/10\n",
      "loss=2.59:  33%|█████████▋                   | 661/1988 [02:55<05:30,  4.01it/s]Eval -- 662: \n",
      "loss = 2.486590623855591\n",
      "update best eval loss to: 2.486591\n",
      "\n",
      "loss=1.77:  67%|██████████████████▋         | 1323/1988 [06:47<02:45,  4.01it/s]Eval -- 1324: \n",
      "loss = 1.5510653257369995\n",
      "update best eval loss to: 1.551065\n",
      "\n",
      "loss=1.78: 100%|███████████████████████████▉| 1985/1988 [10:39<00:00,  4.01it/s]Eval -- 1986: \n",
      "loss = 1.2720142602920532\n",
      "update best eval loss to: 1.272014\n",
      "\n",
      "loss=1.08: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 2/10\n",
      "loss=1.08:  33%|█████████▌                   | 659/1988 [02:54<05:31,  4.01it/s]Eval -- 2648: \n",
      "loss = 1.0992333889007568\n",
      "update best eval loss to: 1.099233\n",
      "\n",
      "loss=0.79:  66%|██████████████████▌         | 1321/1988 [06:47<02:46,  4.01it/s]Eval -- 3310: \n",
      "loss = 1.0562585592269897\n",
      "update best eval loss to: 1.056259\n",
      "\n",
      "loss=0.87: 100%|███████████████████████████▉| 1983/1988 [10:39<00:01,  4.01it/s]Eval -- 3972: \n",
      "loss = 0.9666171073913574\n",
      "update best eval loss to: 0.966617\n",
      "\n",
      "loss=0.97: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 3/10\n",
      "loss=0.57:  33%|█████████▌                   | 657/1988 [02:54<05:31,  4.02it/s]Eval -- 4634: \n",
      "loss = 0.8805143237113953\n",
      "update best eval loss to: 0.880514\n",
      "\n",
      "loss=0.44:  66%|██████████████████▌         | 1319/1988 [06:46<02:46,  4.02it/s]Eval -- 5296: \n",
      "loss = 0.8161096572875977\n",
      "update best eval loss to: 0.816110\n",
      "\n",
      "loss=0.86: 100%|███████████████████████████▉| 1981/1988 [10:38<00:01,  4.01it/s]Eval -- 5958: \n",
      "loss = 0.7653644680976868\n",
      "update best eval loss to: 0.765364\n",
      "\n",
      "loss=0.56: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 4/10\n",
      "loss=0.42:  33%|█████████▌                   | 655/1988 [02:53<05:31,  4.02it/s]Eval -- 6620: \n",
      "loss = 0.7287132143974304\n",
      "update best eval loss to: 0.728713\n",
      "\n",
      "loss=0.89:  66%|██████████████████▌         | 1317/1988 [06:45<02:47,  4.02it/s]Eval -- 7282: \n",
      "loss = 0.6942195296287537\n",
      "update best eval loss to: 0.694220\n",
      "\n",
      "loss=0.78: 100%|███████████████████████████▊| 1979/1988 [10:38<00:02,  4.01it/s]Eval -- 7944: \n",
      "loss = 0.661715567111969\n",
      "update best eval loss to: 0.661716\n",
      "\n",
      "loss=1.07: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 5/10\n",
      "loss=0.48:  33%|█████████▌                   | 653/1988 [02:52<05:32,  4.02it/s]Eval -- 8606: \n",
      "loss = 0.6460818648338318\n",
      "update best eval loss to: 0.646082\n",
      "\n",
      "loss=0.53:  66%|██████████████████▌         | 1315/1988 [06:45<02:47,  4.02it/s]Eval -- 9268: \n",
      "loss = 0.6058059334754944\n",
      "update best eval loss to: 0.605806\n",
      "\n",
      "loss=0.82:  99%|███████████████████████████▊| 1977/1988 [10:37<00:02,  4.02it/s]Eval -- 9930: \n",
      "loss = 0.6018038988113403\n",
      "update best eval loss to: 0.601804\n",
      "\n",
      "loss=1.37: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 6/10\n",
      "loss=0.29:  33%|█████████▍                   | 651/1988 [02:52<05:33,  4.01it/s]Eval -- 10592: \n",
      "loss = 0.549130380153656\n",
      "update best eval loss to: 0.549130\n",
      "\n",
      "loss=0.69:  66%|██████████████████▍         | 1313/1988 [06:44<02:48,  4.02it/s]Eval -- 11254: \n",
      "loss = 0.5480906367301941\n",
      "update best eval loss to: 0.548091\n",
      "\n",
      "loss=0.59:  99%|███████████████████████████▊| 1975/1988 [10:36<00:03,  4.02it/s]Eval -- 11916: \n",
      "loss = 0.524808406829834\n",
      "update best eval loss to: 0.524808\n",
      "\n",
      "loss=0.26: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 7/10\n",
      "loss=0.56:  33%|█████████▍                   | 649/1988 [02:51<05:33,  4.01it/s]Eval -- 12578: \n",
      "loss = 0.507409930229187\n",
      "update best eval loss to: 0.507410\n",
      "\n",
      "loss=0.86:  66%|██████████████████▍         | 1311/1988 [06:44<02:48,  4.02it/s]Eval -- 13240: \n",
      "loss = 0.4815850257873535\n",
      "update best eval loss to: 0.481585\n",
      "\n",
      "loss=0.55:  99%|███████████████████████████▊| 1973/1988 [10:36<00:03,  4.02it/s]Eval -- 13902: \n",
      "loss = 0.4752957224845886\n",
      "update best eval loss to: 0.475296\n",
      "\n",
      "loss=0.73: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 8/10\n",
      "loss=0.90:  33%|█████████▍                   | 647/1988 [02:51<05:33,  4.02it/s]Eval -- 14564: \n",
      "loss = 0.45057329535484314\n",
      "update best eval loss to: 0.450573\n",
      "\n",
      "loss=1.07:  66%|██████████████████▍         | 1309/1988 [06:43<02:49,  4.02it/s]Eval -- 15226: \n",
      "loss = 0.43240952491760254\n",
      "update best eval loss to: 0.432410\n",
      "\n",
      "loss=0.43:  99%|███████████████████████████▊| 1971/1988 [10:36<00:04,  4.01it/s]Eval -- 15888: \n",
      "loss = 0.43734726309776306\n",
      "loss=0.55: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 9/10\n",
      "loss=0.19:  32%|█████████▍                   | 645/1988 [02:50<05:34,  4.01it/s]Eval -- 16550: \n",
      "loss = 0.4106566309928894\n",
      "update best eval loss to: 0.410657\n",
      "\n",
      "loss=1.00:  66%|██████████████████▍         | 1307/1988 [06:43<02:49,  4.01it/s]Eval -- 17212: \n",
      "loss = 0.39273712038993835\n",
      "update best eval loss to: 0.392737\n",
      "\n",
      "loss=1.16:  99%|███████████████████████████▋| 1969/1988 [10:35<00:04,  4.01it/s]Eval -- 17874: \n",
      "loss = 0.3645942807197571\n",
      "update best eval loss to: 0.364594\n",
      "\n",
      "loss=0.96: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "epoch 10/10\n",
      "loss=0.60:  32%|█████████▍                   | 643/1988 [02:50<05:34,  4.02it/s]Eval -- 18536: \n",
      "loss = 0.36459222435951233\n",
      "update best eval loss to: 0.364592\n",
      "\n",
      "loss=0.24:  66%|██████████████████▍         | 1305/1988 [06:42<02:50,  4.02it/s]Eval -- 19198: \n",
      "loss = 0.3312268853187561\n",
      "update best eval loss to: 0.331227\n",
      "\n",
      "loss=0.62:  99%|███████████████████████████▋| 1967/1988 [10:35<00:05,  4.02it/s]Eval -- 19860: \n",
      "loss = 0.33406153321266174\n",
      "loss=1.00: 100%|████████████████████████████| 1988/1988 [11:37<00:00,  2.85it/s]\n",
      "finish training: best_loss = 0.3312268853187561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for args in args_list:\n",
    "    cmd = f\"python3 experiment.py {' '.join(args)}\"\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826f9b7-c7c4-4481-b47b-2da2782a6b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
